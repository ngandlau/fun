{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Helpful) References\n",
    "\n",
    "* http://cs231n.stanford.edu/vecDerivs.pdf\n",
    "* https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The derivative of a vector-valued function $\\vec h(x,W)$  w.r.t. to some weight-matrix $W$\n",
    "\n",
    "Suppose the following setup:\n",
    "\n",
    "<img style=\"max-width:400px;\" src=\"https://i.imgur.com/sG5UBMq.png\"></img>\n",
    "\n",
    "$\\vec h$ is a vector-valued function. We're looking for the derivative $\\frac{\\partial \\vec h}{\\partial W}$. This requires us to calculate partial derivatives of *each* element of h with respect to *each* element of W. In total, we have to compute `3*(2*3)=18` partial derivatives. We can store these 18 derivatives in three (2x3) matrices. These matrices will store the partial derivatives of $\\frac{\\partial h_1}{\\partial W}, \\frac{\\partial h_2}{\\partial W}, \\frac{\\partial h_3}{\\partial W}$.\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/oPLzvtc.png\"></img>\n",
    "\n",
    "When computing the elements of the matrices, we note that the following pattern emerges:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/YZ7T3SQ.png\"></img>\n",
    "\n",
    "Using this, we can compute the partial derivatives:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/thOmApu.png\"></img>\n",
    "\n",
    "These three matrices can now be used to update `W`. To update `W`, we would need to do three steps:\n",
    "\n",
    "1. $W = W - \\text{learning_rate}\\cdot \\frac{\\partial h_1}{\\partial W}$\n",
    "1. $W = W - \\text{learning_rate}\\cdot \\frac{\\partial h_2}{\\partial W}$\n",
    "1. $W = W - \\text{learning_rate}\\cdot \\frac{\\partial h_3}{\\partial W}$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$$\n",
    "W = W - \\text{learning_rate}*(\\frac{\\partial h_1}{\\partial W} + \\frac{\\partial h_2}{\\partial W} + \\frac{\\partial h_3}{\\partial W})\n",
    "$$\n",
    "\n",
    "But there is a trick (see Section 3 of http://cs231n.stanford.edu/vecDerivs.pdf) that (1) simplifies the update-step and (2) helps storing the three matrices more efficiently. For this trick, note that most elements of the three matrices are zero. In fact, the only non-zero elements $\\frac{\\partial h_i}{\\partial W_{jk}}$ of $\\frac{\\partial h_i}{\\partial W}$ are those elements where $i=k$. We could define a new matrix $J_{j,i}=\\frac{\\partial h_i}{\\partial W_{ji}}$ that would store the *same* non-trivial information of all three matrices in a *single* 2D-matrix:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/BK4NXCP.png\"></img>\n",
    "\n",
    "This matrix is the efficiently stored **Jacobian** $J=\\frac{\\partial \\vec h}{\\partial W}$. Using this matrix, the update-step simplifies to:\n",
    "\n",
    "$$\n",
    "W = W - \\text{learning_rate}\\cdot J\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "For example, for input vector $\\vec x=[1, 2]$ the efficiently stored Jacobian is:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/khVlJIY.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 2])\n",
      "W: torch.Size([2, 3])\n",
      "h: torch.Size([1, 3])\n",
      "\n",
      "=> Jacobian/Jacobian Product:\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# The same example in pytorch\n",
    "# https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products\n",
    "\n",
    "import torch\n",
    "x = torch.Tensor([1, 2]).reshape((1, -1))\n",
    "W = torch.randn((2, 3), requires_grad=True)\n",
    "h = x@W\n",
    "print(f\"x: {x.shape}\\nW: {W.shape}\\nh: {h.shape}\")\n",
    "\n",
    "# Normally, we want to compute the gradient of a *scalar* function w.r.t. to some\n",
    "# parameters. In that case we can simply call f.backward()\n",
    "# But here, we want to compute the gradient of a *vector-valued* function w.r.t. some\n",
    "# parameters. In that case f.backward() will compute the *Jacobian product*, and not\n",
    "# the actual gradient, see https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products\n",
    "v = torch.ones_like(h)\n",
    "h.backward(v)\n",
    "J = W.grad # computes v.T@J for input vector v=(v1, ..., vm)\n",
    "print(f\"\\n=> Jacobian/Jacobian Product:\\n{J}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the jacobian product *not* the actual gradient?**\n",
    "\n",
    "See https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products\n",
    "\n",
    "* \"the gradient is subset of the Jacobian.\" \n",
    "* \"the gradient can be seen as special case of the Jacobian, i.e. when the function is scalar\"\n",
    "* \"The Jacobian matrix is the matrix formed by the partial derivatives of a vector function. Its vectors are the gradients of the respective components of the function.\" => The Jacobian stores the GRADIENTS of the components of the function in its columns/rows!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## derivatives in a network \n",
    "\n",
    "* We extend the example above by adding an output layer associated with weights $\\vec w^{(out)}=[w_1^{(out)}, w_2^{(out)}, w_3^{(out)}]$, that takes in $\\vec h=[h_1, h_2, h_3]$ and computes $score=\\vec w^{(out)}\\vec h=w_1^{(out)}h_1 + w_2^{(out)}h_2 + w_3^{(out)}h_3$. \n",
    "* Then `score` can then be pushed through a sigmoid function to be turned into a probability: $prob=\\sigma(score)$. \n",
    "* Then `prob` can be compared with the actual target via some loss function $Loss(prob, target)$.\n",
    "* Then updating the weights $W$ and $w^{(out)}$ is a matter of finding the derivatives `dLoss/dW` and `dLoss/dwout`.\n",
    "\n",
    "Here are the steps visualized:\n",
    "\n",
    "```\n",
    "x = [x1, x2]     # a single observation has 2 features\n",
    "h = [h1, h2, h3] # hidden layer with 3 neurons with states h1, h2, h3\n",
    "```\n",
    "\n",
    "The weights of each of the three neurons correspond to a column\n",
    "\n",
    "```\n",
    "W = [[W11, W12, W13],\n",
    "     [W21, W22, W23]]\n",
    "```\n",
    "\n",
    "Then we can compute the hidden layer\n",
    "\n",
    "```\n",
    "h = xW = [h1, h2, h3] # shape(1x3)\n",
    "```\n",
    "\n",
    "* x:  shape(1x2)\n",
    "* W: shape(2x3)\n",
    "* h: shape(1x3)\n",
    "\n",
    "Then we run `h` through an output layer with a sigmoid activation function. That output layer has weights `v=[v1, v2, v3]`.\n",
    "\n",
    "```\n",
    "out = h@v = [h1 h2 h3] @ [v1 v2 v3] = h1*v1 + h2*v2 + h3*v3\n",
    "```\n",
    "\n",
    "To get a probability, we put the result through a sigmoid activation function:\n",
    "\n",
    "```\n",
    "prob = sigmoid(out)\n",
    "```\n",
    "\n",
    "We can put everything together:\n",
    "\n",
    "\n",
    "```\n",
    "f(x, W, v) = sigmoid(out)\n",
    "           = sigmoid(h @ v)\n",
    "           = sigmoid(xW @ v)\n",
    "```\n",
    "\n",
    "Our goal is now to find `dfdW` and `dfdv` because they are needed to update our weights `W` and `v`.\n",
    "\n",
    "Using the chain rule, we find `dfdW` as follows:\n",
    "\n",
    "```\n",
    "(1)   dprob/dout = sigmoid(out)*sigmoid(1-out)\n",
    "(2.1) dout/dv    = h\n",
    "(2.2) dout/dh    = v\n",
    "(3)   dh/dW = x\n",
    "```\n",
    "\n",
    "Applying the chain rule to get `df/dW` and `df/dv`:\n",
    "\n",
    "```\n",
    "df/dv = dprob/dout * dout/dv \n",
    "      = s(out)*s(1-out) * h\n",
    "\n",
    "df/dW = dprob/dout * dout/dh * dh/dW\n",
    "      = s(out)*s(1-out) * v * x\n",
    "```\n",
    "\n",
    "\n",
    "Now it's unclear how the vector-vector product `v*x = [v1 v2 v3] * [x1 x2]` is defined? In the end, we must get a (2x3) matrix. The only way to get that from a 3D and 2D vector is via the dot product `shape(2x1) @ shape(1x3) = shape(2x3)`, hence `x.T @ v.T`. But it's unclear why...Maybe it has to do with *Jacobians*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) torch.Size([3]) torch.Size([]) torch.Size([]) torch.Size([1])\n",
      "tensor([0.3632], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.functional import relu\n",
    "from torch import sigmoid\n",
    "\n",
    "x = torch.Tensor([1, 2])\n",
    "t = torch.Tensor([1])\n",
    "\n",
    "W = torch.randn((2, 3), requires_grad=True)\n",
    "wout = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# forward\n",
    "h = x@W\n",
    "hact = relu(h)\n",
    "score = hact@wout\n",
    "prob = sigmoid(score)\n",
    "loss = (prob - t)**2\n",
    "\n",
    "print(h.shape, hact.shape, score.shape, prob.shape, loss.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward with storing gradients\n",
    "h = x@W\n",
    "dh_dW = x.repeat(3).reshape((3, 2)).T \n",
    "\n",
    "hact = relu(h)\n",
    "dhact_dh = torch.zeros_like(hact)\n",
    "dhact_dh[hact > 0] = 1.0\n",
    "\n",
    "score = hact@wout\n",
    "dscore_dwout = hact\n",
    "dscore_dhact = wout\n",
    "\n",
    "prob = sigmoid(score)\n",
    "dprob_dscore = sigmoid(score)*(1-sigmoid(score))\n",
    "\n",
    "loss = (prob - t)**2\n",
    "dloss_dprob = 2*(prob-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss/dprob  = tensor([-1.2053], grad_fn=<MulBackward0>)\n",
      "dprob/dscore = 0.23946520686149597\n",
      "dscore/dwout = tensor([2.0583, 1.7744, 1.7216], grad_fn=<ReluBackward0>)\n",
      "\n",
      "wout \t\t= tensor([-1.3355,  1.2879,  0.0274], requires_grad=True)\n",
      "dloss/dwout \t= tensor([-0.5941, -0.5121, -0.4969], grad_fn=<MulBackward0>)\n",
      "wout \t\t= tensor([-0.7415,  1.8001,  0.5243], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# gradients required for updating wout\n",
    "print(f\"dloss/dprob  = {dloss_dprob}\")\n",
    "print(f\"dprob/dscore = {dprob_dscore}\")\n",
    "print(f\"dscore/dwout = {dscore_dwout}\\n\")\n",
    "\n",
    "# applying chain rule\n",
    "dloss_dwout = dloss_dprob * dprob_dscore * dscore_dwout\n",
    "\n",
    "# update wout\n",
    "print(f\"wout \\t\\t= {wout}\\ndloss/dwout \\t= {dloss_dwout}\")\n",
    "wout_new = wout - 1*dloss_dwout\n",
    "print(f\"wout \\t\\t= {wout_new}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dloss/dprob  \t= tensor([-1.2053], grad_fn=<MulBackward0>)\n",
      "dprob/dscore \t= 0.23946520686149597\n",
      "dscore/dhact \t= tensor([-1.3355,  1.2879,  0.0274], requires_grad=True)\n",
      "dhact/dh \t= tensor([1., 1., 1.])\n",
      "dh/dW =\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.]])\n",
      "\n",
      "tensor([[ 0.3855, -0.3717, -0.0079],\n",
      "        [ 0.7709, -0.7435, -0.0158]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# gradients required for updating W\n",
    "print(f\"dloss/dprob  \\t= {dloss_dprob}\")\n",
    "print(f\"dprob/dscore \\t= {dprob_dscore}\")\n",
    "print(f\"dscore/dhact \\t= {dscore_dhact}\")\n",
    "print(f\"dhact/dh \\t= {dhact_dh}\")\n",
    "print(f\"dh/dW =\\n{dh_dW}\\n\")\n",
    "\n",
    "# applying chain rule\n",
    "dloss_dW = dloss_dprob * dprob_dscore * dscore_dhact * dhact_dh * dh_dW\n",
    "print(dloss_dW)\n",
    "\n",
    "# # update W\n",
    "# print(f\"wout \\t\\t= {wout}\\ndloss/dwout \\t= {dloss_dwout}\")\n",
    "# wout = wout - 1*dloss_dwout\n",
    "# print(f\"wout \\t\\t= {wout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch grad: \ttensor([-0.5941, -0.5121, -0.4969])\n",
      "our grad: \ttensor([-0.5941, -0.5121, -0.4969], grad_fn=<MulBackward0>)\n",
      "are they equal? tensor([True, True, True])\n",
      "\n",
      "pytorch grad: \n",
      "tensor([[ 0.3855, -0.3717, -0.0079],\n",
      "        [ 0.7709, -0.7435, -0.0158]])\n",
      "\n",
      "our grad: \n",
      "tensor([[ 0.3855, -0.3717, -0.0079],\n",
      "        [ 0.7709, -0.7435, -0.0158]], grad_fn=<MulBackward0>)\n",
      "\n",
      "are they equal?\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# compare our manually computed derivatives against those calculated by pytorch\n",
    "\n",
    "# zero gradients and re-do forward pass\n",
    "W.grad = None\n",
    "wout.grad = None\n",
    "\n",
    "h = x@W\n",
    "hact = relu(h)\n",
    "score = hact@wout\n",
    "prob = sigmoid(score)\n",
    "loss = (prob - t)**2\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print(\"dW\")\n",
    "print(f\"pytorch grad: \\t{wout.grad}\\nour grad: \\t{dloss_dwout}\")\n",
    "print(f\"are they equal? {wout.grad == dloss_dwout}\")\n",
    "print()\n",
    "print(f\"pytorch grad: \\n{W.grad}\\n\\nour grad: \\n{dloss_dW}\\n\")\n",
    "print(f\"are they equal?\\n{W.grad == dloss_dW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> WE GET THE SAME GRADIENTS AS PYTORCH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
