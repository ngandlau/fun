{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Helpful) References\n",
    "\n",
    "* http://cs231n.stanford.edu/vecDerivs.pdf\n",
    "* https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup \n",
    "\n",
    "```\n",
    "x = [x1, x2]     # a single observation has 2 features\n",
    "h = [h1, h2, h3] # hidden layer with 3 neurons with states h1, h2, h3\n",
    "```\n",
    "\n",
    "The weights of each of the three neurons correspond to a column\n",
    "\n",
    "```\n",
    "W = [[W11, W12, W13],\n",
    "     [W21, W22, W23]]\n",
    "```\n",
    "\n",
    "Then we can compute the hidden layer\n",
    "\n",
    "```\n",
    "h = xW = [h1, h2, h3] # shape(1x3)\n",
    "```\n",
    "\n",
    "* x:  shape(1x2)\n",
    "* W: shape(2x3)\n",
    "* h: shape(1x3)\n",
    "\n",
    "Then we run `h` through an output layer with a sigmoid activation function. That output layer has weights `v=[v1, v2, v3]`.\n",
    "\n",
    "```\n",
    "out = h@v = [h1 h2 h3] @ [v1 v2 v3] = h1*v1 + h2*v2 + h3*v3\n",
    "```\n",
    "\n",
    "To get a probability, we put the result through a sigmoid activation function:\n",
    "\n",
    "```\n",
    "prob = sigmoid(out)\n",
    "```\n",
    "\n",
    "We can put everything together:\n",
    "\n",
    "\n",
    "```\n",
    "f(x, W, v) = sigmoid(out)\n",
    "           = sigmoid(h @ v)\n",
    "           = sigmoid(xW @ v)\n",
    "```\n",
    "\n",
    "Our goal is now to find `dfdW` and `dfdv` because they are needed to update our weights `W` and `v`.\n",
    "\n",
    "Using the chain rule, we find `dfdW` as follows:\n",
    "\n",
    "```\n",
    "(1)   dprob/dout = sigmoid(out)*sigmoid(1-out)\n",
    "(2.1) dout/dv    = h\n",
    "(2.2) dout/dh    = v\n",
    "(3)   dh/dW = x\n",
    "```\n",
    "\n",
    "Applying the chain rule to get `df/dW` and `df/dv`:\n",
    "\n",
    "```\n",
    "df/dv = dprob/dout * dout/dv \n",
    "      = s(out)*s(1-out) * h\n",
    "\n",
    "df/dW = dprob/dout * dout/dh * dh/dW\n",
    "      = s(out)*s(1-out) * v * x\n",
    "```\n",
    "\n",
    "Now it's unclear how the vector-vector product `v*x = [v1 v2 v3] * [x1 x2]` is defined? In the end, we must get a (2x3) matrix. The only way to get that from a 3D and 2D vector is via the dot product `shape(2x1) @ shape(1x3) = shape(2x3)`, hence `x.T @ v.T`. But it's unclear why...Maybe it has to do with *Jacobians*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## example\n",
    "\n",
    "<img style=\"max-width:400px;\" src=\"https://i.imgur.com/sG5UBMq.png\"></img>\n",
    "\n",
    "$\\vec h$ is a vector-valued function. We're looking for the derivative $\\frac{\\partial \\vec h}{\\partial W}$. This requires us to calculate partial derivatives of *each* element of h with respect to *each* element of W. In total, we have to compute `3*(2*3)=18` partial derivatives. We can store these 18 derivatives in three (2x3) matrices. These matrices will store the partial derivatives of $\\frac{\\partial h_1}{\\partial W}, \\frac{\\partial h_2}{\\partial W}, \\frac{\\partial h_3}{\\partial W}$.\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/oPLzvtc.png\"></img>\n",
    "\n",
    "When computing the elements of the matrices, we note that the following pattern emerges:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/YZ7T3SQ.png\"></img>\n",
    "\n",
    "Using this, we can compute the partial derivatives:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/thOmApu.png\"></img>\n",
    "\n",
    "These three matrices can now be used to update `W`. To update `W`, we would need to do three steps:\n",
    "\n",
    "1. $W = W - \\text{learning_rate}\\cdot \\frac{\\partial h_1}{\\partial W}$\n",
    "1. $W = W - \\text{learning_rate}\\cdot \\frac{\\partial h_2}{\\partial W}$\n",
    "1. $W = W - \\text{learning_rate}\\cdot \\frac{\\partial h_3}{\\partial W}$\n",
    "\n",
    "or, equivalently,\n",
    "\n",
    "$$\n",
    "W = W - \\text{learning_rate}*(\\frac{\\partial h_1}{\\partial W} + \\frac{\\partial h_2}{\\partial W} + \\frac{\\partial h_3}{\\partial W})\n",
    "$$\n",
    "\n",
    "But there is a trick (see Section 3 of http://cs231n.stanford.edu/vecDerivs.pdf) that (1) simplifies the update-step and (2) helps storing the three matrices more efficiently. For this trick, note that most elements of the three matrices are zero. In fact, the only non-zero elements $\\frac{\\partial h_i}{\\partial W_{jk}}$ of $\\frac{\\partial h_i}{\\partial W}$ are those elements where $i=k$. We could define a new matrix $J_{j,i}=\\frac{\\partial h_i}{\\partial W_{ji}}$ that would store the *same* non-trivial information of all three matrices in a *single* 2D-matrix:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/BK4NXCP.png\"></img>\n",
    "\n",
    "This matrix is the efficiently stored **Jacobian** $J=\\frac{\\partial \\vec h}{\\partial W}$. Using this matrix, the update-step simplifies to:\n",
    "\n",
    "$$\n",
    "W = W - \\text{learning_rate}\\cdot J\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "For example, for input vector $\\vec x=[1, 2]$ the efficiently stored Jacobian is:\n",
    "\n",
    "<img style=\"max-width:500px;\" src=\"https://i.imgur.com/khVlJIY.png\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([1, 2])\n",
      "W: torch.Size([2, 3])\n",
      "h: torch.Size([1, 3])\n",
      "\n",
      "=> Jacobian/Jacobian Product:\n",
      "tensor([[1., 1., 1.],\n",
      "        [2., 2., 2.]])\n"
     ]
    }
   ],
   "source": [
    "# The same example in pytorch\n",
    "# https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products\n",
    "\n",
    "import torch\n",
    "x = torch.Tensor([1, 2]).reshape((1, -1))\n",
    "W = torch.randn((2, 3), requires_grad=True)\n",
    "h = x@W\n",
    "print(f\"x: {x.shape}\\nW: {W.shape}\\nh: {h.shape}\")\n",
    "\n",
    "# Normally, we want to compute the gradient of a *scalar* function w.r.t. to some\n",
    "# parameters. In that case we can simply call f.backward()\n",
    "# But here, we want to compute the gradient of a *vector-valued* function w.r.t. some\n",
    "# parameters. In that case f.backward() will compute the *Jacobian product*, and not\n",
    "# the actual gradient, see https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products\n",
    "v = torch.ones_like(h)\n",
    "h.backward(v)\n",
    "J = W.grad # computes v.T@J for input vector v=(v1, ..., vm)\n",
    "print(f\"\\n=> Jacobian/Jacobian Product:\\n{J}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why is the jacobian product *not* the actual gradient?**\n",
    "\n",
    "See https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html#optional-reading-tensor-gradients-and-jacobian-products\n",
    "\n",
    "* \"the gradient is subset of the Jacobian.\" \n",
    "* \"the gradient can be seen as special case of the Jacobian, i.e. when the function is scalar\"\n",
    "* \"The Jacobian matrix is the matrix formed by the partial derivatives of a vector function. Its vectors are the gradients of the respective components of the function.\" => The Jacobian stores the GRADIENTS of the components of the function in its columns/rows!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
